{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Overview:\n",
      "   Unnamed: 0  mapped_veh_id       timestamps_UTC        lat       lon  \\\n",
      "0           0            181  2023-08-01 03:44:12  50.769818  3.872114   \n",
      "1           1            143  2023-08-01 06:36:29  51.039993  3.693429   \n",
      "2           2            183  2023-08-24 06:53:54  50.742203  3.602035   \n",
      "3           3            177  2023-08-01 13:53:38  50.930914  5.327132   \n",
      "4           4            143  2023-08-24 07:02:30  51.180773  3.575259   \n",
      "\n",
      "   RS_E_InAirTemp_PC1  RS_E_InAirTemp_PC2  RS_E_OilPress_PC1  \\\n",
      "0                27.0                23.0              255.0   \n",
      "1                33.0                32.0              272.0   \n",
      "2                31.0                33.0              234.0   \n",
      "3                35.0                38.0              220.0   \n",
      "4                41.0                34.0              227.0   \n",
      "\n",
      "   RS_E_OilPress_PC2  RS_E_RPM_PC1  RS_E_RPM_PC2  RS_E_WatTemp_PC1  \\\n",
      "0              238.0         794.0         801.0              83.0   \n",
      "1              324.0         802.0         804.0              78.0   \n",
      "2              182.0         799.0         802.0              82.0   \n",
      "3              244.0         794.0         801.0              77.0   \n",
      "4              282.0         806.0         800.0              85.0   \n",
      "\n",
      "   RS_E_WatTemp_PC2  RS_T_OilTemp_PC1  RS_T_OilTemp_PC2  \n",
      "0              81.0              76.0              77.0  \n",
      "1              78.0              73.0              74.0  \n",
      "2              82.0              85.0              87.0  \n",
      "3              81.0              78.0              82.0  \n",
      "4              78.0              82.0              79.0  \n",
      "\n",
      "Dataset Information:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 100 entries, 0 to 99\n",
      "Data columns (total 15 columns):\n",
      " #   Column              Non-Null Count  Dtype  \n",
      "---  ------              --------------  -----  \n",
      " 0   Unnamed: 0          100 non-null    int64  \n",
      " 1   mapped_veh_id       100 non-null    int64  \n",
      " 2   timestamps_UTC      100 non-null    object \n",
      " 3   lat                 100 non-null    float64\n",
      " 4   lon                 100 non-null    float64\n",
      " 5   RS_E_InAirTemp_PC1  100 non-null    float64\n",
      " 6   RS_E_InAirTemp_PC2  100 non-null    float64\n",
      " 7   RS_E_OilPress_PC1   100 non-null    float64\n",
      " 8   RS_E_OilPress_PC2   100 non-null    float64\n",
      " 9   RS_E_RPM_PC1        100 non-null    float64\n",
      " 10  RS_E_RPM_PC2        100 non-null    float64\n",
      " 11  RS_E_WatTemp_PC1    100 non-null    float64\n",
      " 12  RS_E_WatTemp_PC2    100 non-null    float64\n",
      " 13  RS_T_OilTemp_PC1    100 non-null    float64\n",
      " 14  RS_T_OilTemp_PC2    100 non-null    float64\n",
      "dtypes: float64(12), int64(2), object(1)\n",
      "memory usage: 11.8+ KB\n",
      "None\n",
      "\n",
      "Summary Statistics:\n",
      "       Unnamed: 0  mapped_veh_id         lat         lon  RS_E_InAirTemp_PC1  \\\n",
      "count  100.000000     100.000000  100.000000  100.000000          100.000000   \n",
      "mean    49.500000     154.850000   50.959578    4.324667           35.075000   \n",
      "std     29.011492      26.236829    0.265157    0.668921            8.717037   \n",
      "min      0.000000     102.000000   50.057252    3.575259            0.000000   \n",
      "25%     24.750000     136.000000   50.929888    3.710251           31.000000   \n",
      "50%     49.500000     151.500000   51.035852    4.239056           36.000000   \n",
      "75%     74.250000     181.000000   51.175034    5.112819           40.000000   \n",
      "max     99.000000     194.000000   51.246008    5.541184           56.000000   \n",
      "\n",
      "       RS_E_InAirTemp_PC2  RS_E_OilPress_PC1  RS_E_OilPress_PC2  RS_E_RPM_PC1  \\\n",
      "count          100.000000         100.000000         100.000000    100.000000   \n",
      "mean            34.775000         249.777000         261.832333    932.685000   \n",
      "std              5.950713          82.294219          74.596954    390.184272   \n",
      "min             19.000000           0.000000           6.000000      0.000000   \n",
      "25%             31.000000         213.000000         213.000000    798.000000   \n",
      "50%             35.000000         234.000000         256.500000    801.000000   \n",
      "75%             39.000000         293.750000         295.500000    820.500000   \n",
      "max             56.000000         420.000000         503.000000   1981.000000   \n",
      "\n",
      "       RS_E_RPM_PC2  RS_E_WatTemp_PC1  RS_E_WatTemp_PC2  RS_T_OilTemp_PC1  \\\n",
      "count    100.000000        100.000000         100.00000        100.000000   \n",
      "mean     950.532667         78.125000          80.15500         79.333667   \n",
      "std      365.275172         15.712919           9.58895          8.774792   \n",
      "min        0.000000          0.000000          35.00000         28.000000   \n",
      "25%      799.000000         78.000000          80.00000         77.000000   \n",
      "50%      801.500000         82.000000          82.00000         81.000000   \n",
      "75%      826.250000         85.000000          85.00000         84.000000   \n",
      "max     1988.000000         90.000000          91.00000         93.000000   \n",
      "\n",
      "       RS_T_OilTemp_PC2  \n",
      "count        100.000000  \n",
      "mean          79.298000  \n",
      "std           12.837154  \n",
      "min            0.000000  \n",
      "25%           77.000000  \n",
      "50%           82.000000  \n",
      "75%           87.000000  \n",
      "max           93.000000  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "file_path = \"ar41_for_ulb_mini.csv\"  \n",
    "df = pd.read_csv(file_path, sep=';')\n",
    "\n",
    "# Display the first few rows of the dataset\n",
    "print(\"Dataset Overview:\")\n",
    "print(df.head())\n",
    "\n",
    "# Display basic information about the dataset\n",
    "print(\"\\nDataset Information:\")\n",
    "print(df.info())\n",
    "\n",
    "# Summary statistics for numerical columns\n",
    "print(\"\\nSummary Statistics:\")\n",
    "print(df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exploration:\n",
    "\n",
    "Perform exploratory data analysis (EDA) to understand the distribution of each variable, identify patterns, and gain insights into the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(\"\\nMissing Values:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Explore unique values in categorical columns\n",
    "print(\"\\nUnique Values:\")\n",
    "for column in df.select_dtypes(include='object').columns:\n",
    "    print(f\"{column}: {df[column].unique()}\")\n",
    "\n",
    "# Visualize the distribution of numerical features (you can customize this based on your preferences)\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Example: Distribution of 'RS_E_InAirTemp_PC1'\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(df['RS_E_InAirTemp_PC1'], bins=20, kde=True)\n",
    "plt.title('Distribution of RS_E_InAirTemp_PC1')\n",
    "plt.xlabel('RS_E_InAirTemp_PC1')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced visualizations for numerical features\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Correlation heatmap for numerical features\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(df.corr(), annot=True, cmap='coolwarm', fmt='.2f')\n",
    "plt.title('Correlation Heatmap')\n",
    "plt.show()\n",
    "\n",
    "# Pair plot for selected numerical features\n",
    "numerical_features = ['lat', 'lon', 'RS_E_InAirTemp_PC1', 'RS_E_OilPress_PC1', 'RS_E_RPM_PC1']\n",
    "sns.pairplot(df[numerical_features])\n",
    "plt.suptitle('Pair Plot of Selected Numerical Features', y=1.02)\n",
    "plt.show()\n",
    "\n",
    "# Time series plot for selected features\n",
    "plt.figure(figsize=(14, 6))\n",
    "plt.plot(df['timestamps_UTC'], df['RS_E_InAirTemp_PC1'], label='RS_E_InAirTemp_PC1')\n",
    "plt.plot(df['timestamps_UTC'], df['RS_E_OilPress_PC1'], label='RS_E_OilPress_PC1')\n",
    "plt.title('Time Series Plot of Selected Features')\n",
    "plt.xlabel('Timestamp')\n",
    "plt.ylabel('Values')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32md:\\Polytechnique\\MA2\\H423 - Data Mining\\SNCB\\TrainAnomalyDetection\\try.ipynb Cell 5\u001b[0m line \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Polytechnique/MA2/H423%20-%20Data%20Mining/SNCB/TrainAnomalyDetection/try.ipynb#W5sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Multivariate outlier detection using Mahalanobis distance\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Polytechnique/MA2/H423%20-%20Data%20Mining/SNCB/TrainAnomalyDetection/try.ipynb#W5sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcovariance\u001b[39;00m \u001b[39mimport\u001b[39;00m EllipticEnvelope\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Polytechnique/MA2/H423%20-%20Data%20Mining/SNCB/TrainAnomalyDetection/try.ipynb#W5sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m# Select relevant numerical features for multivariate outlier detection\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Polytechnique/MA2/H423%20-%20Data%20Mining/SNCB/TrainAnomalyDetection/try.ipynb#W5sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m multivariate_features \u001b[39m=\u001b[39m [\u001b[39m'\u001b[39m\u001b[39mlat\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mlon\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mRS_E_InAirTemp_PC1\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mRS_E_OilPress_PC1\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mRS_E_RPM_PC1\u001b[39m\u001b[39m'\u001b[39m]\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\sklearn\\__init__.py:83\u001b[0m\n\u001b[0;32m     69\u001b[0m     \u001b[39m# We are not importing the rest of scikit-learn during the build\u001b[39;00m\n\u001b[0;32m     70\u001b[0m     \u001b[39m# process, as it may not be compiled yet\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     77\u001b[0m     \u001b[39m# later is linked to the OpenMP runtime to make it possible to introspect\u001b[39;00m\n\u001b[0;32m     78\u001b[0m     \u001b[39m# it and importing it first would fail if the OpenMP dll cannot be found.\u001b[39;00m\n\u001b[0;32m     79\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[0;32m     80\u001b[0m         __check_build,  \u001b[39m# noqa: F401\u001b[39;00m\n\u001b[0;32m     81\u001b[0m         _distributor_init,  \u001b[39m# noqa: F401\u001b[39;00m\n\u001b[0;32m     82\u001b[0m     )\n\u001b[1;32m---> 83\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mbase\u001b[39;00m \u001b[39mimport\u001b[39;00m clone\n\u001b[0;32m     84\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_show_versions\u001b[39;00m \u001b[39mimport\u001b[39;00m show_versions\n\u001b[0;32m     86\u001b[0m     __all__ \u001b[39m=\u001b[39m [\n\u001b[0;32m     87\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mcalibration\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m     88\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mcluster\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    129\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mshow_versions\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    130\u001b[0m     ]\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\sklearn\\base.py:19\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_config\u001b[39;00m \u001b[39mimport\u001b[39;00m config_context, get_config\n\u001b[0;32m     18\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mexceptions\u001b[39;00m \u001b[39mimport\u001b[39;00m InconsistentVersionWarning\n\u001b[1;32m---> 19\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m \u001b[39mimport\u001b[39;00m _IS_32BIT\n\u001b[0;32m     20\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_estimator_html_repr\u001b[39;00m \u001b[39mimport\u001b[39;00m estimator_html_repr\n\u001b[0;32m     21\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_metadata_requests\u001b[39;00m \u001b[39mimport\u001b[39;00m _MetadataRequester\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\sklearn\\utils\\__init__.py:22\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_bunch\u001b[39;00m \u001b[39mimport\u001b[39;00m Bunch\n\u001b[0;32m     21\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_estimator_html_repr\u001b[39;00m \u001b[39mimport\u001b[39;00m estimator_html_repr\n\u001b[1;32m---> 22\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_param_validation\u001b[39;00m \u001b[39mimport\u001b[39;00m Interval, validate_params\n\u001b[0;32m     23\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mclass_weight\u001b[39;00m \u001b[39mimport\u001b[39;00m compute_class_weight, compute_sample_weight\n\u001b[0;32m     24\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mdeprecation\u001b[39;00m \u001b[39mimport\u001b[39;00m deprecated\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:15\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mscipy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msparse\u001b[39;00m \u001b[39mimport\u001b[39;00m csr_matrix, issparse\n\u001b[0;32m     14\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_config\u001b[39;00m \u001b[39mimport\u001b[39;00m config_context, get_config\n\u001b[1;32m---> 15\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mvalidation\u001b[39;00m \u001b[39mimport\u001b[39;00m _is_arraylike_not_scalar\n\u001b[0;32m     18\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mInvalidParameterError\u001b[39;00m(\u001b[39mValueError\u001b[39;00m, \u001b[39mTypeError\u001b[39;00m):\n\u001b[0;32m     19\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Custom exception to be raised when the parameter of a class/method/function\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[39m    does not have a valid type or value.\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\sklearn\\utils\\validation.py:25\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m get_config \u001b[39mas\u001b[39;00m _get_config\n\u001b[0;32m     24\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mexceptions\u001b[39;00m \u001b[39mimport\u001b[39;00m DataConversionWarning, NotFittedError, PositiveSpectrumWarning\n\u001b[1;32m---> 25\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_array_api\u001b[39;00m \u001b[39mimport\u001b[39;00m _asarray_with_order, _is_numpy_namespace, get_namespace\n\u001b[0;32m     26\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfixes\u001b[39;00m \u001b[39mimport\u001b[39;00m ComplexWarning\n\u001b[0;32m     27\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_isfinite\u001b[39;00m \u001b[39mimport\u001b[39;00m FiniteStatus, cy_isfinite\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\sklearn\\utils\\_array_api.py:6\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mfunctools\u001b[39;00m \u001b[39mimport\u001b[39;00m wraps\n\u001b[0;32m      5\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mscipy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mspecial\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mspecial\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_config\u001b[39;00m \u001b[39mimport\u001b[39;00m get_config\n\u001b[0;32m      9\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mfixes\u001b[39;00m \u001b[39mimport\u001b[39;00m parse_version\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\scipy\\special\\__init__.py:775\u001b[0m\n\u001b[0;32m    772\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m _ufuncs\n\u001b[0;32m    773\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_ufuncs\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n\u001b[1;32m--> 775\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m _basic\n\u001b[0;32m    776\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_basic\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n\u001b[0;32m    778\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_logsumexp\u001b[39;00m \u001b[39mimport\u001b[39;00m logsumexp, softmax, log_softmax\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\scipy\\special\\_basic.py:16\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_ufuncs\u001b[39;00m \u001b[39mimport\u001b[39;00m (mathieu_a, mathieu_b, iv, jv, gamma,\n\u001b[0;32m     14\u001b[0m                       psi, hankel1, hankel2, yv, kv, poch, binom)\n\u001b[0;32m     15\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m _specfun\n\u001b[1;32m---> 16\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_comb\u001b[39;00m \u001b[39mimport\u001b[39;00m _comb_int\n\u001b[0;32m     19\u001b[0m __all__ \u001b[39m=\u001b[39m [\n\u001b[0;32m     20\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mai_zeros\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m     21\u001b[0m     \u001b[39m'\u001b[39m\u001b[39massoc_laguerre\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     77\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mzeta\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m     78\u001b[0m ]\n\u001b[0;32m     81\u001b[0m \u001b[39m# mapping k to last n such that factorialk(n, k) < np.iinfo(np.int64).max\u001b[39;00m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:405\u001b[0m, in \u001b[0;36mparent\u001b[1;34m(self)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Multivariate outlier detection using Mahalanobis distance\n",
    "from sklearn.covariance import EllipticEnvelope\n",
    "\n",
    "# Select relevant numerical features for multivariate outlier detection\n",
    "multivariate_features = ['lat', 'lon', 'RS_E_InAirTemp_PC1', 'RS_E_OilPress_PC1', 'RS_E_RPM_PC1']\n",
    "\n",
    "# Fit Elliptic Envelope model\n",
    "envelope = EllipticEnvelope()\n",
    "df['multivariate_outlier'] = envelope.fit_predict(df[multivariate_features].values.reshape(-1, len(multivariate_features)))\n",
    "\n",
    "# Visualize multivariate outliers\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x='lon', y='lat', hue='multivariate_outlier', data=df, palette='viridis')\n",
    "plt.title('Multivariate Outlier Detection')\n",
    "plt.show()\n",
    "\n",
    "# Drop multivariate outliers\n",
    "df = df[df['multivariate_outlier'] != -1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare the distribution of numerical features between normal and anomaly instances\n",
    "selected_features = ['RS_E_InAirTemp_PC1', 'RS_E_OilPress_PC1', 'RS_E_RPM_PC1']\n",
    "for feature in selected_features:\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.kdeplot(df[df['label'] == 0][feature], label='Normal', shade=True)\n",
    "    sns.kdeplot(df[df['label'] == 1][feature], label='Anomaly', shade=True)\n",
    "    plt.title(f'Distribution Comparison for {feature}')\n",
    "    plt.xlabel(feature)\n",
    "    plt.ylabel('Density')\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing:\n",
    "\n",
    "Handle missing values, outliers, or any inconsistencies in the data.\n",
    "Convert timestamp strings to datetime objects for time-based analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'timestamps_UTC' to datetime format\n",
    "df['timestamps_UTC'] = pd.to_datetime(df['timestamps_UTC'])\n",
    "\n",
    "# Handle missing values (you can choose different strategies based on your analysis)\n",
    "# Example: Fill missing numerical values with the mean\n",
    "df.fillna(df.mean(), inplace=True)\n",
    "\n",
    "# Check for outliers and consider handling them based on your analysis\n",
    "\n",
    "# Ensure data types are appropriate for each column\n",
    "# Example: Convert columns to appropriate data types\n",
    "df['mapped_veh_id'] = df['mapped_veh_id'].astype('category')\n",
    "\n",
    "# Any other preprocessing steps based on your exploration\n",
    "\n",
    "# Save the preprocessed dataset if needed\n",
    "# df.to_csv(\"path/to/your/preprocessed_dataset.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32md:\\Polytechnique\\MA2\\H423 - Data Mining\\SNCB\\TrainAnomalyDetection\\try.ipynb Cell 8\u001b[0m line \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Polytechnique/MA2/H423%20-%20Data%20Mining/SNCB/TrainAnomalyDetection/try.ipynb#X11sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Handling Outliers using Z-score\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Polytechnique/MA2/H423%20-%20Data%20Mining/SNCB/TrainAnomalyDetection/try.ipynb#X11sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mscipy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mstats\u001b[39;00m \u001b[39mimport\u001b[39;00m zscore\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Polytechnique/MA2/H423%20-%20Data%20Mining/SNCB/TrainAnomalyDetection/try.ipynb#X11sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m# Example: Detect and remove outliers for 'RS_E_InAirTemp_PC1'\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Polytechnique/MA2/H423%20-%20Data%20Mining/SNCB/TrainAnomalyDetection/try.ipynb#X11sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m z_scores \u001b[39m=\u001b[39m zscore(df[\u001b[39m'\u001b[39m\u001b[39mRS_E_InAirTemp_PC1\u001b[39m\u001b[39m'\u001b[39m])\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\scipy\\stats\\__init__.py:608\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[39m.. _statsrefmanual:\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    603\u001b[0m \n\u001b[0;32m    604\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    606\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_warnings_errors\u001b[39;00m \u001b[39mimport\u001b[39;00m (ConstantInputWarning, NearConstantInputWarning,\n\u001b[0;32m    607\u001b[0m                                DegenerateDataWarning, FitError)\n\u001b[1;32m--> 608\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_stats_py\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n\u001b[0;32m    609\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_variation\u001b[39;00m \u001b[39mimport\u001b[39;00m variation\n\u001b[0;32m    610\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mdistributions\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\scipy\\stats\\_stats_py.py:46\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mscipy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mspecial\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mspecial\u001b[39;00m\n\u001b[0;32m     45\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mscipy\u001b[39;00m \u001b[39mimport\u001b[39;00m linalg\n\u001b[1;32m---> 46\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m distributions\n\u001b[0;32m     47\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m _mstats_basic \u001b[39mas\u001b[39;00m mstats_basic\n\u001b[0;32m     48\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_stats_mstats_common\u001b[39;00m \u001b[39mimport\u001b[39;00m (_find_repeats, linregress, theilslopes,\n\u001b[0;32m     49\u001b[0m                                    siegelslopes)\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\scipy\\stats\\distributions.py:8\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m#\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[39m# Author:  Travis Oliphant  2002-2011 with contributions from\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[39m#          SciPy Developers 2004-2011\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[39m#       instead of `git blame -Lxxx,+x`.\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[39m#\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_distn_infrastructure\u001b[39;00m \u001b[39mimport\u001b[39;00m (rv_discrete, rv_continuous, rv_frozen)  \u001b[39m# noqa: F401\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m _continuous_distns\n\u001b[0;32m     11\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m _discrete_distns\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\scipy\\stats\\_distn_infrastructure.py:23\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mscipy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mspecial\u001b[39;00m \u001b[39mimport\u001b[39;00m comb, entr\n\u001b[0;32m     21\u001b[0m \u001b[39m# for root finding for continuous distribution ppf, and maximum likelihood\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[39m# estimation\u001b[39;00m\n\u001b[1;32m---> 23\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mscipy\u001b[39;00m \u001b[39mimport\u001b[39;00m optimize\n\u001b[0;32m     25\u001b[0m \u001b[39m# for functions of continuous distributions (e.g. moments, entropy, cdf)\u001b[39;00m\n\u001b[0;32m     26\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mscipy\u001b[39;00m \u001b[39mimport\u001b[39;00m integrate\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1229\u001b[0m, in \u001b[0;36m_handle_fromlist\u001b[1;34m(module, fromlist, import_, recursive)\u001b[0m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\scipy\\__init__.py:202\u001b[0m, in \u001b[0;36m__getattr__\u001b[1;34m(name)\u001b[0m\n\u001b[0;32m    200\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getattr__\u001b[39m(name):\n\u001b[0;32m    201\u001b[0m     \u001b[39mif\u001b[39;00m name \u001b[39min\u001b[39;00m submodules:\n\u001b[1;32m--> 202\u001b[0m         \u001b[39mreturn\u001b[39;00m _importlib\u001b[39m.\u001b[39;49mimport_module(\u001b[39mf\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mscipy.\u001b[39;49m\u001b[39m{\u001b[39;49;00mname\u001b[39m}\u001b[39;49;00m\u001b[39m'\u001b[39;49m)\n\u001b[0;32m    203\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    204\u001b[0m         \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\importlib\\__init__.py:126\u001b[0m, in \u001b[0;36mimport_module\u001b[1;34m(name, package)\u001b[0m\n\u001b[0;32m    124\u001b[0m             \u001b[39mbreak\u001b[39;00m\n\u001b[0;32m    125\u001b[0m         level \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m--> 126\u001b[0m \u001b[39mreturn\u001b[39;00m _bootstrap\u001b[39m.\u001b[39;49m_gcd_import(name[level:], package, level)\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\scipy\\optimize\\__init__.py:411\u001b[0m\n\u001b[0;32m    409\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_optimize\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n\u001b[0;32m    410\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_minimize\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n\u001b[1;32m--> 411\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_root\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n\u001b[0;32m    412\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_root_scalar\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n\u001b[0;32m    413\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_minpack_py\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\scipy\\optimize\\_root.py:15\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mwarnings\u001b[39;00m \u001b[39mimport\u001b[39;00m warn\n\u001b[0;32m     14\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_optimize\u001b[39;00m \u001b[39mimport\u001b[39;00m MemoizeJac, OptimizeResult, _check_unknown_options\n\u001b[1;32m---> 15\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_minpack_py\u001b[39;00m \u001b[39mimport\u001b[39;00m _root_hybr, leastsq\n\u001b[0;32m     16\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_spectral\u001b[39;00m \u001b[39mimport\u001b[39;00m _root_df_sane\n\u001b[0;32m     17\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m _nonlin \u001b[39mas\u001b[39;00m nonlin\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\scipy\\optimize\\_minpack_py.py:13\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mscipy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_lib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_util\u001b[39;00m \u001b[39mimport\u001b[39;00m getfullargspec_no_self \u001b[39mas\u001b[39;00m _getfullargspec\n\u001b[0;32m     12\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_optimize\u001b[39;00m \u001b[39mimport\u001b[39;00m OptimizeResult, _check_unknown_options, OptimizeWarning\n\u001b[1;32m---> 13\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_lsq\u001b[39;00m \u001b[39mimport\u001b[39;00m least_squares\n\u001b[0;32m     14\u001b[0m \u001b[39m# from ._lsq.common import make_strictly_feasible\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_lsq\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mleast_squares\u001b[39;00m \u001b[39mimport\u001b[39;00m prepare_bounds\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\scipy\\optimize\\_lsq\\__init__.py:3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m\"\"\"This module contains least-squares algorithms.\"\"\"\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mleast_squares\u001b[39;00m \u001b[39mimport\u001b[39;00m least_squares\n\u001b[1;32m----> 3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mlsq_linear\u001b[39;00m \u001b[39mimport\u001b[39;00m lsq_linear\n\u001b[0;32m      5\u001b[0m __all__ \u001b[39m=\u001b[39m [\u001b[39m'\u001b[39m\u001b[39mleast_squares\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mlsq_linear\u001b[39m\u001b[39m'\u001b[39m]\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\scipy\\optimize\\_lsq\\lsq_linear.py:10\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mscipy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39moptimize\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_minimize\u001b[39;00m \u001b[39mimport\u001b[39;00m Bounds\n\u001b[0;32m      9\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mcommon\u001b[39;00m \u001b[39mimport\u001b[39;00m in_bounds, compute_grad\n\u001b[1;32m---> 10\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mtrf_linear\u001b[39;00m \u001b[39mimport\u001b[39;00m trf_linear\n\u001b[0;32m     11\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mbvls\u001b[39;00m \u001b[39mimport\u001b[39;00m bvls\n\u001b[0;32m     14\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mprepare_bounds\u001b[39m(bounds, n):\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1176\u001b[0m, in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1147\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:690\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[1;34m(spec)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:936\u001b[0m, in \u001b[0;36mexec_module\u001b[1;34m(self, module)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:1032\u001b[0m, in \u001b[0;36mget_code\u001b[1;34m(self, fullname)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:1130\u001b[0m, in \u001b[0;36mget_data\u001b[1;34m(self, path)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Handling Outliers using Z-score\n",
    "from scipy.stats import zscore\n",
    "\n",
    "# Example: Detect and remove outliers for 'RS_E_InAirTemp_PC1'\n",
    "z_scores = zscore(df['RS_E_InAirTemp_PC1'])\n",
    "outliers = (z_scores > 3) | (z_scores < -3)\n",
    "df = df[~outliers]\n",
    "\n",
    "# Feature Engineering\n",
    "# Example: Create a new feature representing the hour of the day\n",
    "df['hour_of_day'] = df['timestamps_UTC'].dt.hour\n",
    "\n",
    "# Impute missing values using advanced methods (e.g., interpolation)\n",
    "df.interpolate(method='linear', inplace=True)\n",
    "\n",
    "# Encoding categorical variables if needed\n",
    "# Example: One-hot encoding for 'mapped_veh_id'\n",
    "df = pd.get_dummies(df, columns=['mapped_veh_id'], prefix='veh_id')\n",
    "\n",
    "# Scaling numerical features if needed\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "numerical_columns = ['lat', 'lon', 'RS_E_InAirTemp_PC1', 'RS_E_OilPress_PC1', 'RS_E_RPM_PC1']\n",
    "df[numerical_columns] = scaler.fit_transform(df[numerical_columns])\n",
    "\n",
    "# Any other advanced preprocessing steps based on your analysis\n",
    "\n",
    "# Save the advanced preprocessed dataset if needed\n",
    "# df.to_csv(\"path/to/your/advanced_preprocessed_dataset.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract time-related features from the timestamp\n",
    "df['day_of_week'] = df['timestamps_UTC'].dt.dayofweek\n",
    "df['day_of_month'] = df['timestamps_UTC'].dt.day\n",
    "df['month'] = df['timestamps_UTC'].dt.month\n",
    "df['year'] = df['timestamps_UTC'].dt.year\n",
    "\n",
    "# Lag features for time series analysis\n",
    "for feature in ['RS_E_InAirTemp_PC1', 'RS_E_OilPress_PC1', 'RS_E_RPM_PC1']:\n",
    "    df[f'{feature}_lag_1'] = df[feature].shift(1)\n",
    "    df[f'{feature}_rolling_mean'] = df[feature].rolling(window=3).mean()\n",
    "\n",
    "# Handling imbalanced data (if applicable)\n",
    "# Example: Resample to balance the number of normal and anomaly instances\n",
    "from sklearn.utils import resample\n",
    "\n",
    "# Separate normal and anomaly instances\n",
    "normal_instances = df[df['label'] == 0]\n",
    "anomaly_instances = df[df['label'] == 1]\n",
    "\n",
    "# Upsample minority class (anomalies) to balance the dataset\n",
    "anomaly_upsampled = resample(anomaly_instances, replace=True, n_samples=len(normal_instances), random_state=42)\n",
    "df_balanced = pd.concat([normal_instances, anomaly_upsampled])\n",
    "\n",
    "# Save the advanced preprocessed dataset if needed\n",
    "# df.to_csv(\"path/to/your/final_dataset.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use dimensionality reduction techniques like PCA to reduce feature dimensionality\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Specify the number of components based on explained variance ratio\n",
    "num_components = 5\n",
    "pca = PCA(n_components=num_components)\n",
    "pca_result = pca.fit_transform(df[selected_features])\n",
    "\n",
    "# Visualize explained variance ratio\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, num_components + 1), pca.explained_variance_ratio_.cumsum(), marker='o', linestyle='--')\n",
    "plt.title('Explained Variance Ratio for PCA Components')\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Cumulative Explained Variance Ratio')\n",
    "plt.show()\n",
    "\n",
    "# Transform the dataset with the selected number of components\n",
    "df_pca = pd.concat([df, pd.DataFrame(pca_result, columns=[f'PCA_{i}' for i in range(1, num_components + 1)])], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use advanced imputation methods like KNN imputation\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "# Select features for imputation\n",
    "features_for_imputation = ['RS_E_InAirTemp_PC1', 'RS_E_OilPress_PC1', 'RS_E_RPM_PC1']\n",
    "\n",
    "# Initialize the imputer\n",
    "imputer = KNNImputer(n_neighbors=5)\n",
    "\n",
    "# Impute missing values using KNN imputation\n",
    "df[features_for_imputation] = imputer.fit_transform(df[features_for_imputation])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enrichment with Weather Data:\n",
    "Data Source Integration:\n",
    "\n",
    "Integrate weather data from sources like OpenWeatherMap or other available APIs.\n",
    "Match weather data with timestamps from the train dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# Function to fetch weather data from OpenWeatherMap API\n",
    "def get_weather_data(api_key, lat, lon, timestamp):\n",
    "    base_url = \"http://api.openweathermap.org/data/2.5/onecall\"\n",
    "    params = {\n",
    "        'lat': lat,\n",
    "        'lon': lon,\n",
    "        'exclude': 'current,minutely,hourly,alerts',\n",
    "        'appid': api_key\n",
    "    }\n",
    "\n",
    "    response = requests.get(base_url, params=params)\n",
    "    weather_data = response.json()\n",
    "\n",
    "    # Extract relevant weather features\n",
    "    if 'daily' in weather_data:\n",
    "        for day in weather_data['daily']:\n",
    "            # Extract desired features (e.g., temperature, humidity, etc.)\n",
    "            # Add these features to the dataset based on the timestamp\n",
    "\n",
    "    return df  # Return the enriched dataset\n",
    "\n",
    "# Specify your OpenWeatherMap API key\n",
    "api_key = \"your_openweathermap_api_key\"\n",
    "\n",
    "# Iterate through rows and enrich the dataset with weather data\n",
    "for index, row in df.iterrows():\n",
    "    lat, lon, timestamp = row['lat'], row['lon'], row['timestamps_UTC']\n",
    "    df.at[index, 'weather_enriched'] = get_weather_data(api_key, lat, lon, timestamp)\n",
    "\n",
    "# Save the dataset with weather enrichment\n",
    "# df.to_csv(\"path/to/your/enriched_dataset.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# Function to fetch weather data from Dark Sky API\n",
    "def get_darksky_data(api_key, lat, lon, timestamp):\n",
    "    base_url = f\"https://api.darksky.net/forecast/{api_key}/{lat},{lon},{timestamp}?exclude=currently,minutely,hourly,alerts\"\n",
    "    response = requests.get(base_url)\n",
    "    weather_data = response.json()\n",
    "\n",
    "    # Extract relevant weather features\n",
    "    if 'daily' in weather_data and 'data' in weather_data['daily']:\n",
    "        for day in weather_data['daily']['data']:\n",
    "            # Extract desired features (e.g., temperature, humidity, etc.)\n",
    "            # Add these features to the dataset based on the timestamp\n",
    "\n",
    "    return df  # Return the enriched dataset\n",
    "\n",
    "# Specify your Dark Sky API key\n",
    "darksky_api_key = \"your_darksky_api_key\"\n",
    "\n",
    "# Iterate through rows and enrich the dataset with weather data from Dark Sky\n",
    "for index, row in df.iterrows():\n",
    "    lat, lon, timestamp = row['lat'], row['lon'], row['timestamps_UTC']\n",
    "    df.at[index, 'weather_enriched_darksky'] = get_darksky_data(darksky_api_key, lat, lon, timestamp)\n",
    "\n",
    "# Save the dataset with Dark Sky weather enrichment\n",
    "# df.to_csv(\"path/to/your/enriched_dataset_darksky.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Developing and Comparing Anomaly Detection Methods:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select Anomaly Detection Algorithm (Isolation Forest):\n",
    "Isolation Forest Algorithm:\n",
    "The Isolation Forest algorithm is an ensemble-based anomaly detection method that isolates anomalies by randomly selecting features and splitting data points. Anomalies are expected to be isolated in fewer splits, making them easier to detect.\n",
    "\n",
    "Implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "# Assuming your features include both original train features and weather features\n",
    "# Replace this with your actual list of feature names\n",
    "selected_features = ['RS_E_InAirTemp_PC1', 'RS_E_OilPress_PC1', 'RS_E_RPM_PC1', 'temperature', 'humidity']\n",
    "\n",
    "# Select features and label for training\n",
    "X = df[selected_features]\n",
    "y = df['label']  # Assuming you have a 'label' column indicating normal (0) and anomaly (1)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize and train the Isolation Forest model\n",
    "isolation_forest_model = IsolationForest(contamination='auto', random_state=42)\n",
    "isolation_forest_model.fit(X_train)\n",
    "\n",
    "# Predict anomalies on the testing set\n",
    "y_pred = isolation_forest_model.predict(X_test)\n",
    "\n",
    "# Map predictions to 0 (normal) and 1 (anomaly)\n",
    "y_pred_mapped = np.where(y_pred == -1, 1, 0)\n",
    "\n",
    "# Evaluate the performance of the model\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_mapped))\n",
    "\n",
    "# Evaluate AUC-ROC\n",
    "auc_roc = roc_auc_score(y_test, y_pred_mapped)\n",
    "print(f\"AUC-ROC Score: {auc_roc}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enhanced Isolation Forest Implementation:\n",
    "1. Feature Scaling:\n",
    "Scale numerical features to ensure that all features contribute equally to the model.\n",
    "This is particularly important if the features have different scales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Initialize the scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit and transform the scaler on the training data\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Initialize and train the Isolation Forest model on scaled data\n",
    "isolation_forest_model_scaled = IsolationForest(contamination='auto', random_state=42)\n",
    "isolation_forest_model_scaled.fit(X_train_scaled)\n",
    "\n",
    "# Predict anomalies on the scaled testing set\n",
    "y_pred_scaled = isolation_forest_model_scaled.predict(X_test_scaled)\n",
    "y_pred_mapped_scaled = np.where(y_pred_scaled == -1, 1, 0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Grid Search for Hyperparameter Tuning (Optional):\n",
    "Perform a grid search to find optimal hyperparameters for the Isolation Forest model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define hyperparameters to search\n",
    "param_grid = {'n_estimators': [50, 100, 200], 'max_samples': ['auto', 100, 200]}\n",
    "\n",
    "# Initialize Isolation Forest model\n",
    "isolation_forest_model_tuned = IsolationForest(contamination='auto', random_state=42)\n",
    "\n",
    "# Initialize GridSearchCV\n",
    "grid_search = GridSearchCV(isolation_forest_model_tuned, param_grid, scoring='roc_auc', cv=3)\n",
    "grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_params = grid_search.best_params_\n",
    "print(f\"Best Hyperparameters: {best_params}\")\n",
    "\n",
    "# Use the best model for predictions on the testing set\n",
    "best_model = grid_search.best_estimator_\n",
    "y_pred_tuned = best_model.predict(X_test_scaled)\n",
    "y_pred_mapped_tuned = np.where(y_pred_tuned == -1, 1, 0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Visualization of Anomalies:\n",
    "Visualize the anomalies detected by the Isolation Forest model on the testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming you have a 'timestamps_UTC' column in your testing set\n",
    "timestamps_test = X_test['timestamps_UTC']\n",
    "\n",
    "# Plot anomalies over time\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.scatter(timestamps_test, y_pred_mapped_scaled, c=y_pred_mapped_scaled, cmap='viridis')\n",
    "plt.title('Anomalies Detected by Isolation Forest (Scaled)')\n",
    "plt.xlabel('Timestamp')\n",
    "plt.ylabel('Anomaly Prediction (0: Normal, 1: Anomaly)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Approach 2: Local Outlier Factor (LOF)\n",
    "LOF Algorithm:\n",
    "The Local Outlier Factor (LOF) is a density-based anomaly detection method. It measures the local density deviation of a data point with respect to its neighbors.\n",
    "\n",
    "Implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "\n",
    "# Initialize the LOF model\n",
    "lof_model = LocalOutlierFactor(contamination='auto')\n",
    "\n",
    "# Fit the LOF model on the training data\n",
    "lof_model.fit(X_train_scaled)\n",
    "\n",
    "# Predict anomalies on the scaled testing set\n",
    "y_pred_lof = lof_model.fit_predict(X_test_scaled)\n",
    "y_pred_mapped_lof = np.where(y_pred_lof == -1, 1, 0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Approach 3: One-Class SVM\n",
    "One-Class SVM Algorithm:\n",
    "One-Class SVM is a support vector machine algorithm designed for binary classification with only one class of interest (inliers). It learns a boundary around normal instances.\n",
    "\n",
    "Implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import OneClassSVM\n",
    "\n",
    "# Initialize the One-Class SVM model\n",
    "svm_model = OneClassSVM(nu=0.05)  # You may need to adjust the 'nu' parameter based on your data\n",
    "\n",
    "# Fit the One-Class SVM model on the training data\n",
    "svm_model.fit(X_train_scaled)\n",
    "\n",
    "# Predict anomalies on the scaled testing set\n",
    "y_pred_svm = svm_model.predict(X_test_scaled)\n",
    "y_pred_mapped_svm = np.where(y_pred_svm == -1, 1, 0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Approach 4: Autoencoders\n",
    "Autoencoder Algorithm:\n",
    "Autoencoders are a type of artificial neural network used for unsupervised learning. They learn a compressed representation of the input data and can be used for anomaly detection by reconstructing normal instances accurately.\n",
    "\n",
    "Implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "# Build the Autoencoder model\n",
    "model = Sequential([\n",
    "    Dense(16, activation='relu', input_shape=(X_train_scaled.shape[1],)),\n",
    "    Dense(8, activation='relu'),\n",
    "    Dense(16, activation='relu'),\n",
    "    Dense(X_train_scaled.shape[1], activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# Fit the Autoencoder model on the training data\n",
    "model.fit(X_train_scaled, X_train_scaled, epochs=50, batch_size=32, validation_split=0.2)\n",
    "\n",
    "# Reconstruct data and calculate reconstruction error\n",
    "X_pred = model.predict(X_test_scaled)\n",
    "reconstruction_error = np.mean(np.square(X_test_scaled - X_pred), axis=1)\n",
    "\n",
    "# Define a threshold for anomaly detection (e.g., mean + 3 * standard deviation)\n",
    "threshold = np.mean(reconstruction_error) + 3 * np.std(reconstruction_error)\n",
    "\n",
    "# Map predictions to 0 (normal) and 1 (anomaly)\n",
    "y_pred_autoencoder = np.where(reconstruction_error > threshold, 1, 0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate Performance:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation for LOF:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Evaluate LOF\n",
    "print(\"Evaluation for LOF:\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred_mapped_lof))\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_mapped_lof))\n",
    "print(\"\\nAUC-ROC Score:\")\n",
    "auc_roc_lof = roc_auc_score(y_test, y_pred_mapped_lof)\n",
    "print(auc_roc_lof)\n",
    "\n",
    "# Visualization for LOF\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.scatter(timestamps_test, y_pred_mapped_lof, c=y_pred_mapped_lof, cmap='viridis')\n",
    "plt.title('Anomalies Detected by LOF')\n",
    "plt.xlabel('Timestamp')\n",
    "plt.ylabel('Anomaly Prediction (0: Normal, 1: Anomaly)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation for One-Class SVM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate One-Class SVM\n",
    "print(\"\\nEvaluation for One-Class SVM:\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred_mapped_svm))\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_mapped_svm))\n",
    "print(\"\\nAUC-ROC Score:\")\n",
    "auc_roc_svm = roc_auc_score(y_test, y_pred_mapped_svm)\n",
    "print(auc_roc_svm)\n",
    "\n",
    "# Visualization for One-Class SVM\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.scatter(timestamps_test, y_pred_mapped_svm, c=y_pred_mapped_svm, cmap='viridis')\n",
    "plt.title('Anomalies Detected by One-Class SVM')\n",
    "plt.xlabel('Timestamp')\n",
    "plt.ylabel('Anomaly Prediction (0: Normal, 1: Anomaly)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation for Autoencoders:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Autoencoders\n",
    "print(\"\\nEvaluation for Autoencoders:\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred_autoencoder))\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_autoencoder))\n",
    "print(\"\\nAUC-ROC Score:\")\n",
    "auc_roc_autoencoder = roc_auc_score(y_test, y_pred_autoencoder)\n",
    "print(auc_roc_autoencoder)\n",
    "\n",
    "# Visualization for Autoencoders\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.scatter(timestamps_test, y_pred_autoencoder, c=y_pred_autoencoder, cmap='viridis')\n",
    "plt.title('Anomalies Detected by Autoencoders')\n",
    "plt.xlabel('Timestamp')\n",
    "plt.ylabel('Anomaly Prediction (0: Normal, 1: Anomaly)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Comparison:\n",
    "First, let's compare the selected anomaly detection models (Isolation Forest, LOF, One-Class SVM, and Autoencoders) based on key evaluation metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation for Isolation Forest (Assuming you've selected it)\n",
    "print(\"Evaluation for Isolation Forest:\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred_mapped_scaled))\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_mapped_scaled))\n",
    "print(\"\\nAUC-ROC Score:\")\n",
    "auc_roc_isolation_forest = roc_auc_score(y_test, y_pred_mapped_scaled)\n",
    "print(auc_roc_isolation_forest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dashboard Development with Plotly and Dash:\n",
    "Dashboard Features:\n",
    "Time Series Chart:\n",
    "\n",
    "Displays the anomalies over time, allowing you to observe patterns.\n",
    "Geographical Distribution Map:\n",
    "\n",
    "Represents the anomalies on a geographical map using latitude and longitude.\n",
    "Metrics Summary:\n",
    "\n",
    "Presents key evaluation metrics (precision, recall, F1-score, AUC-ROC) for a quick overview.\n",
    "Additional Steps:\n",
    "Precision, Recall, F1-Score, AUC-ROC:\n",
    "\n",
    "Before running the dashboard, calculate precision, recall, F1-score, and AUC-ROC for the selected anomaly detection approach (Isolation Forest, LOF, One-Class SVM, or Autoencoders).\n",
    "Replace Column Names:\n",
    "\n",
    "Replace the placeholder column names in the code with your actual column names.\n",
    "Run the Dashboard:\n",
    "\n",
    "Run the provided Dash app code locally.\n",
    "Open a web browser and navigate to http://127.0.0.1:8050/ to view the dashboard.\n",
    "Dashboard Interaction (Optional):\n",
    "\n",
    "Enhance the dashboard with additional interactive features or customizations based on your requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dash\n",
    "from dash import dcc, html\n",
    "from dash.dependencies import Input, Output\n",
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming 'timestamps_UTC', 'lat', 'lon', 'label' columns are present in your testing set\n",
    "# Replace these with your actual column names\n",
    "df_dashboard = pd.DataFrame({\n",
    "    'Timestamp': timestamps_test,\n",
    "    'Latitude': X_test['lat'],\n",
    "    'Longitude': X_test['lon'],\n",
    "    'Anomaly': y_test\n",
    "})\n",
    "\n",
    "# Initialize Dash app\n",
    "app = dash.Dash(__name__)\n",
    "\n",
    "# Define layout\n",
    "app.layout = html.Div(children=[\n",
    "    html.H1(\"Train Anomaly Detection Dashboard\"),\n",
    "    \n",
    "    # Line chart of anomalies over time\n",
    "    dcc.Graph(\n",
    "        id='time-series-chart',\n",
    "        figure=px.line(df_dashboard, x='Timestamp', y='Anomaly', title='Anomalies Over Time')\n",
    "    ),\n",
    "    \n",
    "    # Scatter map of anomalies\n",
    "    dcc.Graph(\n",
    "        id='map-chart',\n",
    "        figure=px.scatter_geo(\n",
    "            df_dashboard, \n",
    "            lat='Latitude', \n",
    "            lon='Longitude', \n",
    "            color='Anomaly', \n",
    "            title='Geographical Distribution of Anomalies'\n",
    "        )\n",
    "    ),\n",
    "    \n",
    "    # Metrics summary\n",
    "    html.Div([\n",
    "        html.H3(\"Metrics Summary\"),\n",
    "        html.Div([\n",
    "            html.P(f\"Precision: {precision:.2f}\"),\n",
    "            html.P(f\"Recall: {recall:.2f}\"),\n",
    "            html.P(f\"F1-Score: {f1_score:.2f}\"),\n",
    "            html.P(f\"AUC-ROC: {auc_roc:.2f}\")\n",
    "        ])\n",
    "    ])\n",
    "])\n",
    "\n",
    "# Assume you have predictions for each model: y_pred_mapped_scaled, y_pred_mapped_lof, y_pred_mapped_svm, y_pred_autoencoder\n",
    "\n",
    "# Create a DataFrame for model predictions\n",
    "df_predictions = pd.DataFrame({\n",
    "    'Timestamp': timestamps_test,\n",
    "    'Isolation_Forest': y_pred_mapped_scaled,\n",
    "    'LOF': y_pred_mapped_lof,\n",
    "    'One_Class_SVM': y_pred_mapped_svm,\n",
    "    'Autoencoder': y_pred_autoencoder\n",
    "})\n",
    "\n",
    "# Add a new section to the layout for model comparison\n",
    "app.layout = html.Div(children=[\n",
    "    html.H1(\"Train Anomaly Detection Dashboard\"),\n",
    "    \n",
    "    # ... (previous layout components)\n",
    "    \n",
    "    # Model comparison line chart\n",
    "    dcc.Graph(\n",
    "        id='model-comparison-chart',\n",
    "        figure=px.line(df_predictions, x='Timestamp', y=df_predictions.columns[1:],\n",
    "                       title='Model Comparison - Anomalies Over Time')\n",
    "    ),\n",
    "    \n",
    "    # Model comparison metrics summary\n",
    "    html.Div([\n",
    "        html.H3(\"Model Comparison Metrics\"),\n",
    "        html.Div([\n",
    "            html.P(f\"Isolation Forest AUC-ROC: {auc_roc_isolation_forest:.2f}\"),\n",
    "            # Add metrics for other models as needed\n",
    "        ])\n",
    "    ])\n",
    "])\n",
    "\n",
    "# Run the app\n",
    "if __name__ == '__main__':\n",
    "    app.run_server(debug=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simulating Streaming Data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "# Function to generate a new observation\n",
    "def generate_observation():\n",
    "    timestamp = datetime.utcnow()\n",
    "    lat = np.random.uniform(50, 52)\n",
    "    lon = np.random.uniform(3, 5)\n",
    "    features = np.random.normal(0, 1, 14)  # Assuming 14 features for the example\n",
    "    return pd.DataFrame([[timestamp, lat, lon] + list(features)],\n",
    "                        columns=['timestamps_UTC', 'lat', 'lon'] +\n",
    "                                [f'feature_{i}' for i in range(14)])\n",
    "\n",
    "# Simulate streaming data\n",
    "while True:\n",
    "    new_observation = generate_observation()\n",
    "    \n",
    "    # Preprocess the new observation (assuming the same preprocessing steps as the training set)\n",
    "    new_observation_scaled = scaler.transform(new_observation.iloc[:, 1:])\n",
    "    \n",
    "    # Predict anomaly using the selected model (assuming Isolation Forest for this example)\n",
    "    new_observation_pred = isolation_forest_model_scaled.predict(new_observation_scaled)\n",
    "    \n",
    "    # Report anomaly if detected\n",
    "    if new_observation_pred == -1:\n",
    "        print(f\"Anomaly detected at {new_observation['timestamps_UTC'].values[0]}\")\n",
    "        # Update the dashboard dynamically (e.g., using Dash callback)\n",
    "\n",
    "    # Sleep for a short duration (simulating streaming interval)\n",
    "    time.sleep(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additional Steps:\n",
    "Adjust Model and Features:\n",
    "\n",
    "Replace the anomaly detection model and features in the script based on the chosen approach (Isolation Forest, LOF, One-Class SVM, or Autoencoders).\n",
    "Streaming Environment:\n",
    "\n",
    "Run the streaming data script in a separate terminal or as a background process to continuously generate new observations.\n",
    "Real-Time Anomaly Detection:\n",
    "\n",
    "As new observations are generated, the script will preprocess and predict anomalies in real-time using the selected model.\n",
    "Monitoring and Logging (Optional):\n",
    "\n",
    "Implement logging or monitoring mechanisms to track anomalies and system performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary Dash components\n",
    "from dash.dependencies import Input, Output\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "# Assume 'app' is your existing Dash app\n",
    "\n",
    "# Add a callback function to update the real-time anomaly chart\n",
    "@app.callback(Output('real-time-chart', 'figure'),\n",
    "              [Input('interval-component', 'n_intervals')])\n",
    "def update_real_time_chart(n):\n",
    "    # Fetch the latest anomalies from a shared storage or database\n",
    "    latest_anomalies = get_latest_anomalies()  # Implement this function based on your storage\n",
    "\n",
    "    # Plot the real-time anomalies\n",
    "    fig = px.line(latest_anomalies, x='timestamps_UTC', y='Anomaly', title='Real-Time Anomalies')\n",
    "    \n",
    "    return fig\n",
    "\n",
    "# Add an interval component for periodic updates\n",
    "app.layout = html.Div(children=[\n",
    "    html.H1(\"Train Anomaly Detection Dashboard\"),\n",
    "\n",
    "    # ... (previous layout components)\n",
    "\n",
    "    # Real-time anomaly chart\n",
    "    dcc.Graph(id='real-time-chart'),\n",
    "\n",
    "    # Interval component for periodic updates\n",
    "    dcc.Interval(\n",
    "        id='interval-component',\n",
    "        interval=10 * 1000,  # in milliseconds\n",
    "        n_intervals=0\n",
    "    )\n",
    "])\n",
    "\n",
    "# Run the app\n",
    "if __name__ == '__main__':\n",
    "    app.run_server(debug=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shared Storage for Anomalies:\n",
    "\n",
    "Implement a shared storage mechanism (database, cache, etc.) to store and retrieve the latest anomalies.\n",
    "Streaming Data and Dashboard Integration:\n",
    "\n",
    "Run the streaming data script and the Dash app simultaneously. The dashboard should now update in real-time when anomalies are detected."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
