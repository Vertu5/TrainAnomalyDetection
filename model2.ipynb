{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask as da\n",
    "import dask.dataframe as daskdf\n",
    "import dask.array as daa\n",
    "import dask.distributed as dd\n",
    "import dask.datasets as ds\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "import seaborn as sns\n",
    "import sklearn as sk\n",
    "import numpy as np\n",
    "import dask_ml.preprocessing as dm_pre\n",
    "import dask_ml.cluster as dm_cluster\n",
    "import dask_geopandas as dg\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import geopandas as gpd\n",
    "from scipy.stats import pearsonr\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase Four: Modeling. \n",
    "This phase involves selecting, applying, and tuning various statistical or machine learning models to your prepared data. Here's what typically happens in this phase:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load preprocessed data\n",
    "\"\"\"data head look like this: \n",
    "   mapped_veh_id       timestamps_UTC  RS_E_InAirTemp_PC1  RS_E_InAirTemp_PC2  \\\n",
    "0          112.0  2023-08-01 11:42:55              309.15              313.15   \n",
    "1          179.0  2023-08-24 17:48:07              314.15              303.15   \n",
    "2          190.0  2023-08-01 12:27:13              307.15              313.15   \n",
    "3          179.0  2023-08-24 18:18:05              311.15              304.15   \n",
    "4          122.0  2023-08-01 12:53:12              303.15              304.15   \n",
    "\n",
    "   RS_E_OilPress_PC1  RS_E_OilPress_PC2  RS_E_RPM_PC1  RS_E_RPM_PC2  \\\n",
    "0              276.0              248.0         798.0         797.0   \n",
    "1              220.0              227.0         796.0         798.0   \n",
    "2              376.0              317.0        1236.0        1214.0   \n",
    "3              220.0              220.0         802.0         799.0   \n",
    "4              193.0              269.0         801.0         798.0   \n",
    "\n",
    "   RS_E_WatTemp_PC1  RS_E_WatTemp_PC2  RS_T_OilTemp_PC1  RS_T_OilTemp_PC2  \\\n",
    "0            346.15            355.15            351.15            355.15   \n",
    "1            355.15            356.15            352.15            355.15   \n",
    "2            356.15            350.15            357.15            354.15   \n",
    "3            355.15            355.15            353.15            355.15   \n",
    "4            352.15            352.15            350.15            350.15   \n",
    "\n",
    "   tempmax  tempmin  temp  \n",
    "0     19.7     15.4  17.4  \n",
    "...\n",
    "2     19.7     15.1  17.2  \n",
    "3     24.1     15.9  19.8  \n",
    "4     19.7     15.4  17.4  \n",
    "\"\"\"\n",
    "# Load data\n",
    "def load_data():\n",
    "    print(\"Loading data...\")\n",
    "    start = time.time()\n",
    "    df = daskdf.read_csv(\"preprocessed.csv\")\n",
    "    end = time.time()\n",
    "    print(\"Data loaded in {} seconds\".format(end-start))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Data loaded in 0.09375190734863281 seconds\n",
      "   mapped_veh_id       timestamps_UTC  RS_E_InAirTemp_PC1  RS_E_InAirTemp_PC2  \\\n",
      "0          112.0  2023-08-01 11:42:55              309.15              313.15   \n",
      "1          179.0  2023-08-24 17:48:07              314.15              303.15   \n",
      "2          190.0  2023-08-01 12:27:13              307.15              313.15   \n",
      "3          179.0  2023-08-24 18:18:05              311.15              304.15   \n",
      "4          122.0  2023-08-01 12:53:12              303.15              304.15   \n",
      "\n",
      "   RS_E_OilPress_PC1  RS_E_OilPress_PC2  RS_E_RPM_PC1  RS_E_RPM_PC2  \\\n",
      "0              276.0              248.0         798.0         797.0   \n",
      "1              220.0              227.0         796.0         798.0   \n",
      "2              376.0              317.0        1236.0        1214.0   \n",
      "3              220.0              220.0         802.0         799.0   \n",
      "4              193.0              269.0         801.0         798.0   \n",
      "\n",
      "   RS_E_WatTemp_PC1  RS_E_WatTemp_PC2  RS_T_OilTemp_PC1  RS_T_OilTemp_PC2  \\\n",
      "0            346.15            355.15            351.15            355.15   \n",
      "1            355.15            356.15            352.15            355.15   \n",
      "2            356.15            350.15            357.15            354.15   \n",
      "3            355.15            355.15            353.15            355.15   \n",
      "4            352.15            352.15            350.15            350.15   \n",
      "\n",
      "   tempmax  tempmin  temp  \n",
      "0     19.7     15.4  17.4  \n",
      "1     24.1     15.9  19.8  \n",
      "2     19.7     15.1  17.2  \n",
      "3     24.1     15.9  19.8  \n",
      "4     19.7     15.4  17.4  \n",
      "14814399\n"
     ]
    }
   ],
   "source": [
    "ddf = load_data()\n",
    "ddf = ddf.drop(columns=[\"Unnamed: 0\"])\n",
    "print(ddf.head())\n",
    "print(len(ddf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unsuppervised Modeling techniques selection\n",
    "# 1. Isolation Forest: Effective for high-dimensional datasets. It isolates anomalies instead of modeling normal points, which is efficient when anomalies are rare.\n",
    "\n",
    "# 2. Local Outlier Factor (LOF): It is an unsupervised outlier detection method which computes the local density deviation of a given data point with respect to its neighbors. It considers as outliers the samples that have a substantially lower density than their neighbors. can less effectively handle datasets with varying densities and lof is not well suited for high dimensional data. and it cannot handle the noise data. \n",
    "\n",
    "# 3. One-Class SVM: It is an unsupervised algorithm that learns a decision function for novelty detection: classifying new data as similar or different to the training set. It is based on the idea that a normal data point is surrounded by similar data points, while abnormal data points are far from their neighbors. It is effective in high dimensional spaces and it is memory efficient. It is not suited for large datasets and it does not perform well when the proportion of outliers is high.\n",
    "\n",
    "# 4. DBSCAN: It is a density-based clustering algorithm that groups together points that are close to each other based on a distance measurement (usually Euclidean distance). It is effective for data which contains clusters of similar density. It is not suited for datasets with varying densities and it cannot handle noisy data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Local Outlier Factor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 19\u001b[0m\n\u001b[0;32m     17\u001b[0m scaler \u001b[38;5;241m=\u001b[39m StandardScaler()\n\u001b[0;32m     18\u001b[0m features_to_normalize \u001b[38;5;241m=\u001b[39m ddf\u001b[38;5;241m.\u001b[39mcolumns\n\u001b[1;32m---> 19\u001b[0m df[features_to_normalize] \u001b[38;5;241m=\u001b[39m scaler\u001b[38;5;241m.\u001b[39mfit_transform(\u001b[43mdf\u001b[49m[features_to_normalize])\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# Initialize LOF\u001b[39;00m\n\u001b[0;32m     22\u001b[0m lof \u001b[38;5;241m=\u001b[39m LocalOutlierFactor(n_neighbors\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m, contamination\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.00001\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "#  LOF Model Implementation\n",
    "\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# feature selection\n",
    "# Since LOF is a distance-based model, it is not suitable for high-dimensional datasets. So we will drop highly correlated features.\n",
    "# Drop highly correlated features based on Pearso matrix previously calculated in data preprocessing\n",
    "ddf = ddf.drop(columns=['RS_E_InAirTemp_PC2', 'RS_E_OilPress_PC2', 'RS_E_RPM_PC2', 'RS_E_WatTemp_PC2', 'RS_T_OilTemp_PC2'])\n",
    "\n",
    "# Drop non-feature columns\n",
    "ddf = ddf.drop(columns=['tempmax', 'tempmin'])\n",
    "\n",
    "ddf['timestamps_UTC'] = (ddf['timestamps_UTC'].astype('datetime64[ns]') - pd.Timestamp(\"1970-01-01\")) // pd.Timedelta('1s')\n",
    "\n",
    "# Normalize the feature columns\n",
    "scaler = StandardScaler()\n",
    "features_to_normalize = ddf.columns\n",
    "df[features_to_normalize] = scaler.fit_transform(df[features_to_normalize])\n",
    "\n",
    "# Initialize LOF\n",
    "lof = LocalOutlierFactor(n_neighbors=20, contamination=0.00001)\n",
    "\n",
    "# Fit the model (excluding non-feature columns)\n",
    "df['lof_anomalies'] = lof.fit_predict(df[features_to_normalize])\n",
    "df['lof_anomalies'] = df['lof_anomalies'].map({1: 0, -1: 1})  # Convert to 0 for normal, 1 for anomaly\n",
    "\n",
    "# Review the detected anomalies\n",
    "anomalies_lof = df[df['lof_anomalies'] == 1]\n",
    "print(anomalies_lof)\n",
    "\n",
    "# Output the anomalies to a new CSV for further analysis\n",
    "anomalies_lof.to_csv('anomalies_lof.csv')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the anomalies with legend of each feature\n",
    "for feature in df.drop(columns=['lof_anomalies']).columns:\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.title(feature)\n",
    "    plt.plot(df[feature],'b.', label='Normal')\n",
    "    plt.plot(anomalies_lof[feature], 'ro', label='Anomaly')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on sample 1\n",
    "sample1_predictions = lof.fit_predict(sample1_features)\n",
    "sample1_predictions = pd.Series(sample1_predictions, index=sample1_features.index).map({1: 0, -1: 1})\n",
    "sample1_metrics = {\n",
    "    'Accuracy': sk.accuracy_score(sample1_labels, sample1_predictions),\n",
    "    'Precision': sk.precision_score(sample1_labels, sample1_predictions),\n",
    "    'Recall': sk.recall_score(sample1_labels, sample1_predictions),\n",
    "    'F1 Score': sk.f1_score(sample1_labels, sample1_predictions),\n",
    "    'Confusion Matrix': sk.confusion_matrix(sample1_labels, sample1_predictions)\n",
    "}\n",
    "\n",
    "# Evaluate the model on sample 2\n",
    "sample2_predictions = lof.fit_predict(sample2_features)\n",
    "sample2_predictions = pd.Series(sample2_predictions, index=sample2_features.index).map({1: 0, -1: 1})\n",
    "sample2_metrics = {\n",
    "    'Accuracy': sk.accuracy_score(sample2_labels, sample2_predictions),\n",
    "    'Precision': sk.precision_score(sample2_labels, sample2_predictions),\n",
    "    'Recall': sk.recall_score(sample2_labels, sample2_predictions),\n",
    "    'F1 Score': sk.f1_score(sample2_labels, sample2_predictions),\n",
    "    'Confusion Matrix': sk.confusion_matrix(sample2_labels, sample2_predictions)\n",
    "}\n",
    "\n",
    "# Print the results\n",
    "print('Sample 1 Metrics:', sample1_metrics)\n",
    "print('\\nSample 2 Metrics:', sample2_metrics)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
