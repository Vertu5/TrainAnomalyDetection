{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask as da\n",
    "import dask.dataframe as daskdf\n",
    "import dask.array as daa\n",
    "import dask.distributed as dd\n",
    "import dask.datasets as ds\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "import seaborn as sns\n",
    "import sklearn as sk\n",
    "import numpy as np\n",
    "import dask_ml.preprocessing as dm_pre\n",
    "import dask_ml.cluster as dm_cluster\n",
    "import dask_geopandas as dg\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import geopandas as gpd\n",
    "from scipy.stats import pearsonr\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Data loaded in 0.16307973861694336 seconds\n"
     ]
    }
   ],
   "source": [
    "\"\"\"data look like this: ;mapped_veh_id;timestamps_UTC;lat;lon;RS_E_InAirTemp_PC1;RS_E_InAirTemp_PC2;RS_E_OilPress_PC1;RS_E_OilPress_PC2;RS_E_RPM_PC1;RS_E_RPM_PC2;RS_E_WatTemp_PC1;RS_E_WatTemp_PC2;RS_T_OilTemp_PC1;RS_T_OilTemp_PC2\n",
    "0;181;2023-08-01 03:44:12;50.7698183;3.8721144;27.0;23.0;255.0;238.0;794.0;801.0;83.0;81.0;76.0;77.0\n",
    "1;143;2023-08-01 06:36:29;51.0399934;3.6934285;33.0;32.0;272.0;324.0;802.0;804.0;78.0;78.0;73.0;74.0\n",
    "2;183;2023-08-24 06:53:54;50.7422026;3.6020347;31.0;33.0;234.0;182.0;799.0;802.0;82.0;82.0;85.0;87.0\"\"\"\n",
    "# Load data\n",
    "def load_data():\n",
    "    print(\"Loading data...\")\n",
    "    start = time.time()\n",
    "    df = daskdf.read_csv(\"ar41_for_ulb.csv\", sep=';')\n",
    "    end = time.time()\n",
    "    print(\"Data loaded in {} seconds\".format(end-start))\n",
    "    return df\n",
    "\n",
    "ddf = load_data()\n",
    "ddfprepprep = ddf.drop(columns=['Unnamed: 0'])\n",
    "# convert all temperatures from celcius to kelvin\n",
    "ddf['RS_E_InAirTemp_PC1'] = ddf['RS_E_InAirTemp_PC1'] + 273.15\n",
    "ddf['RS_E_InAirTemp_PC2'] = ddf['RS_E_InAirTemp_PC2'] + 273.15\n",
    "ddf['RS_E_WatTemp_PC1'] = ddf['RS_E_WatTemp_PC1'] + 273.15\n",
    "ddf['RS_E_WatTemp_PC2'] = ddf['RS_E_WatTemp_PC2'] + 273.15\n",
    "ddf['RS_T_OilTemp_PC1'] = ddf['RS_T_OilTemp_PC1'] + 273.15\n",
    "ddf['RS_T_OilTemp_PC2'] = ddf['RS_T_OilTemp_PC2'] + 273.15\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 3: Data Preparation\n",
    "This phase covers constructing the final data set for modeling tools. Steps include selecting data, cleansing data, constructing data, integrating data, and formatting data.\n",
    "\n",
    "Select Data:\n",
    "\n",
    "Decide on data based on relevance, quality, and technical constraints.\n",
    "Explain data inclusions/exclusions and prioritize attributes.\n",
    "Clean Data:\n",
    "\n",
    "Ensure data cleanliness, address missing values using appropriate techniques.\n",
    "Document how quality problems were addressed.\n",
    "Construct Data:\n",
    "\n",
    "Develop new records or derived attributes, considering modeling needs.\n",
    "Example: Create \"income per head\" as a derived attribute.\n",
    "Integrate Data:\n",
    "\n",
    "Combine information from multiple tables or records.\n",
    "Perform aggregations to summarize information.\n",
    "Format Data:\n",
    "\n",
    "Change data format or design to suit modeling tools.\n",
    "Example: Trim strings, reorganize information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select Data: Filtering out all data before january 2023\n",
    "def select_data(df):\n",
    "    ddf = df[df['timestamps_UTC'] >= '2023-01-01']\n",
    "    # to drop the erratic data and keep only those in 10-90 percentile for every feature\n",
    "    for col in ddf.columns:\n",
    "        if col not in ['timestamps_UTC', 'lat', 'lon', 'mapped_veh_id']:\n",
    "            ddf = ddf[(ddf[col] > ddf[col].quantile(0.0001)) & (ddf[col] < ddf[col].quantile(0.9999))]\n",
    "\n",
    "    return ddf\n",
    "\n",
    "ddfprep = select_data(ddf)\n",
    "\n",
    "# filter out all data out of belgium\n",
    "def filter_belgium(df):\n",
    "    ddf = df[(df['lat'] > 50.0) & (df['lat'] < 51.59) & (df['lon'] > 3.2) & (df['lon'] < 5.89)]\n",
    "    return ddf\n",
    "\n",
    "ddfprep = filter_belgium(ddfprep)\n",
    "#index fix \n",
    "ddfprep = ddfprep.reset_index(drop=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "## AS seen in exploratory analysis, we have nan values only for PC2 on 12726 rows over 17M rows so we can drop them\n",
    "ddfprep = ddfprep.drop_duplicates()\n",
    "ddfprep = ddfprep.dropna()\n",
    "\n",
    "#print (len (ddfprep)) \n",
    "#print(ddf.head())\n",
    "\n",
    "## TODO: Construct Data if needed here "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will cluster the lan and lon and associate to each row the name of a city centroid of the cluster \n",
    "def categorize_city(df):\n",
    "    conditions = [\n",
    "        (df['lat'] < 50.522345) & (df['lon'] < 4.586875),  # Charleroi\n",
    "        (df['lat'] > 50.522345) & (df['lon'] < 4.073960),  # Gent\n",
    "        ((df['lat'] >= 50.522345) & (df['lat'] <= 51.042345) & (4.073960 <= df['lon']) & (df['lon'] < 4.786476) ),  # Brussels\n",
    "        ((df['lat'] > 51.042345) & (4.073960 < df['lon']) & (df['lon'] < 4.786476) ), # Antwerp\n",
    "        ((df['lat'] >= 50.522345) & (4.786476 <= df['lon'])), # Hasselt\n",
    "    ]\n",
    "    choices = ['Charleroi', 'Gent', 'Brussels', 'Antwerp', 'Hasselt']\n",
    "    df['city'] = np.select(conditions, choices, default='Namur')  # Namur as default\n",
    "    return df\n",
    "\n",
    "# Apply the function to each partition of the Dask DataFrame\n",
    "ddfprep = ddfprep.map_partitions(categorize_city)\n",
    "\n",
    "#ddfprep = ddfprep.drop_duplicates()\n",
    "#ddfprep = ddfprep.dropna()\n",
    "# if column unnamed: 0 is present, drop it\n",
    "if 'Unnamed: 0' in ddfprep.columns:\n",
    "    ddfprep = ddfprep.drop(columns=['Unnamed: 0'])\n",
    "    \n",
    "#print(ddfprep.head())\n",
    "#print(len(ddfprep))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading external data...\n",
      "External data loaded in 0.036942 seconds\n",
      "Index(['mapped_veh_id', 'timestamps_UTC', 'RS_E_InAirTemp_PC1',\n",
      "       'RS_E_InAirTemp_PC2', 'RS_E_OilPress_PC1', 'RS_E_OilPress_PC2',\n",
      "       'RS_E_RPM_PC1', 'RS_E_RPM_PC2', 'RS_E_WatTemp_PC1', 'RS_E_WatTemp_PC2',\n",
      "       'RS_T_OilTemp_PC1', 'RS_T_OilTemp_PC2', 'tempmax', 'tempmin', 'temp'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Integrate external data \n",
    "# here we will join external weather data with our data \n",
    "# for that we will perform join over the city and timestamp.\n",
    "\n",
    "# Load external data\n",
    "def load_external_data():\n",
    "    print(\"Loading external data...\")\n",
    "    start = pd.Timestamp.now()\n",
    "    df = daskdf.read_csv(\"weather.csv\")\n",
    "    end = pd.Timestamp.now()\n",
    "    print(\"External data loaded in {} seconds\".format((end-start).total_seconds()))\n",
    "    return df\n",
    "\n",
    "external_df = load_external_data()\n",
    "\n",
    "# Prepare for merge by creating a 'date' column\n",
    "ddfprep['date'] = daskdf.to_datetime(daskdf.to_datetime(ddfprep['timestamps_UTC']).dt.date)\n",
    "#print(ddfprep.head())\n",
    "external_df['date'] = daskdf.to_datetime(external_df['date'])\n",
    "\n",
    "# Merge the DataFrames on 'city' and 'date'\n",
    "merged_df = ddfprep.merge(external_df, on=['city', 'date'], how='left')\n",
    "\n",
    "# Drop columns that are not needed anymore\n",
    "columns_to_drop = ['lat', 'lon', 'date', 'city', 'longitude', 'latitude']\n",
    "merged_df = merged_df.drop(columns=columns_to_drop)\n",
    "\n",
    "merged_df = merged_df.dropna()\n",
    "merged_df = merged_df.drop_duplicates()\n",
    "\n",
    "# index fix\n",
    "merged_df = merged_df.reset_index(drop=True)\n",
    "print(merged_df.columns)\n",
    "\n",
    "# save the preprocessed data\n",
    "#ddfprep.to_csv('preprocessed_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dask DataFrame Structure:\n",
      "                   RS_E_InAirTemp_PC1 RS_E_InAirTemp_PC2 RS_E_OilPress_PC1 RS_E_OilPress_PC2 RS_E_RPM_PC1 RS_E_RPM_PC2 RS_E_WatTemp_PC1 RS_E_WatTemp_PC2 RS_T_OilTemp_PC1 RS_T_OilTemp_PC2  tempmax  tempmin     temp\n",
      "npartitions=1                                                                                                                                                                                                        \n",
      "RS_E_InAirTemp_PC1            float64            float64           float64           float64      float64      float64          float64          float64          float64          float64  float64  float64  float64\n",
      "tempmin                           ...                ...               ...               ...          ...          ...              ...              ...              ...              ...      ...      ...      ...\n",
      "Dask Name: corr, 143 graph layers\n",
      "Dask DataFrame Structure:\n",
      "              mapped_veh_id RS_E_InAirTemp_PC1 RS_E_InAirTemp_PC2 RS_E_OilPress_PC1 RS_E_OilPress_PC2 RS_E_RPM_PC1 RS_E_RPM_PC2 RS_E_WatTemp_PC1 RS_E_WatTemp_PC2 RS_T_OilTemp_PC1 RS_T_OilTemp_PC2  tempmax  tempmin     temp\n",
      "npartitions=1                                                                                                                                                                                                                 \n",
      "                    float64            float64            float64           float64           float64      float64      float64          float64          float64          float64          float64  float64  float64  float64\n",
      "                        ...                ...                ...               ...               ...          ...          ...              ...              ...              ...              ...      ...      ...      ...\n",
      "Dask Name: describe-numeric, 202 graph layers\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 419. MiB for an array with shape (13, 4223651) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 14\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# plot the correlation matrix\u001b[39;00m\n\u001b[0;32m     13\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m10\u001b[39m))\n\u001b[1;32m---> 14\u001b[0m \u001b[43msns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheatmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcorr_matrix\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mannot\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcmap\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcoolwarm\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m plt\u001b[38;5;241m.\u001b[39mtitle(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCorrelation Matrix\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     16\u001b[0m plt\u001b[38;5;241m.\u001b[39mshow()\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\seaborn\\matrix.py:446\u001b[0m, in \u001b[0;36mheatmap\u001b[1;34m(data, vmin, vmax, cmap, center, robust, annot, fmt, annot_kws, linewidths, linecolor, cbar, cbar_kws, cbar_ax, square, xticklabels, yticklabels, mask, ax, **kwargs)\u001b[0m\n\u001b[0;32m    365\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Plot rectangular data as a color-encoded matrix.\u001b[39;00m\n\u001b[0;32m    366\u001b[0m \n\u001b[0;32m    367\u001b[0m \u001b[38;5;124;03mThis is an Axes-level function and will draw the heatmap into the\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    443\u001b[0m \n\u001b[0;32m    444\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    445\u001b[0m \u001b[38;5;66;03m# Initialize the plotter object\u001b[39;00m\n\u001b[1;32m--> 446\u001b[0m plotter \u001b[38;5;241m=\u001b[39m \u001b[43m_HeatMapper\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvmin\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvmax\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcmap\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcenter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrobust\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mannot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfmt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    447\u001b[0m \u001b[43m                      \u001b[49m\u001b[43mannot_kws\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcbar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcbar_kws\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxticklabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    448\u001b[0m \u001b[43m                      \u001b[49m\u001b[43myticklabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    450\u001b[0m \u001b[38;5;66;03m# Add the pcolormesh kwargs here\u001b[39;00m\n\u001b[0;32m    451\u001b[0m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlinewidths\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m linewidths\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\seaborn\\matrix.py:109\u001b[0m, in \u001b[0;36m_HeatMapper.__init__\u001b[1;34m(self, data, vmin, vmax, cmap, center, robust, annot, fmt, annot_kws, cbar, cbar_kws, xticklabels, yticklabels, mask)\u001b[0m\n\u001b[0;32m    107\u001b[0m     plot_data \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mvalues\n\u001b[0;32m    108\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 109\u001b[0m     plot_data \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    110\u001b[0m     data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(plot_data)\n\u001b[0;32m    112\u001b[0m \u001b[38;5;66;03m# Validate the mask and convert to DataFrame\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\dask\\dataframe\\core.py:618\u001b[0m, in \u001b[0;36m_Frame.__array__\u001b[1;34m(self, dtype, **kwargs)\u001b[0m\n\u001b[0;32m    617\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__array__\u001b[39m(\u001b[38;5;28mself\u001b[39m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 618\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_computed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    619\u001b[0m     x \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_computed)\n\u001b[0;32m    620\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\dask\\base.py:342\u001b[0m, in \u001b[0;36mDaskMethodsMixin.compute\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    318\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    319\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Compute this dask collection\u001b[39;00m\n\u001b[0;32m    320\u001b[0m \n\u001b[0;32m    321\u001b[0m \u001b[38;5;124;03m    This turns a lazy Dask collection into its in-memory equivalent.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    340\u001b[0m \u001b[38;5;124;03m    dask.compute\u001b[39;00m\n\u001b[0;32m    341\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 342\u001b[0m     (result,) \u001b[38;5;241m=\u001b[39m \u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraverse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    343\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\dask\\base.py:628\u001b[0m, in \u001b[0;36mcompute\u001b[1;34m(traverse, optimize_graph, scheduler, get, *args, **kwargs)\u001b[0m\n\u001b[0;32m    625\u001b[0m     postcomputes\u001b[38;5;241m.\u001b[39mappend(x\u001b[38;5;241m.\u001b[39m__dask_postcompute__())\n\u001b[0;32m    627\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m shorten_traceback():\n\u001b[1;32m--> 628\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[43mschedule\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdsk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    630\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m repack([f(r, \u001b[38;5;241m*\u001b[39ma) \u001b[38;5;28;01mfor\u001b[39;00m r, (f, a) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(results, postcomputes)])\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\toolz\\functoolz.py:628\u001b[0m, in \u001b[0;36mpipe\u001b[1;34m(data, *funcs)\u001b[0m\n\u001b[0;32m    608\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\" Pipe a value through a sequence of functions\u001b[39;00m\n\u001b[0;32m    609\u001b[0m \n\u001b[0;32m    610\u001b[0m \u001b[38;5;124;03mI.e. ``pipe(data, f, g, h)`` is equivalent to ``h(g(f(data)))``\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    625\u001b[0m \u001b[38;5;124;03m    thread_last\u001b[39;00m\n\u001b[0;32m    626\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    627\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m func \u001b[38;5;129;01min\u001b[39;00m funcs:\n\u001b[1;32m--> 628\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    629\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:2249\u001b[0m, in \u001b[0;36m_merge_blocks\u001b[1;34m(blocks, dtype, can_consolidate)\u001b[0m\n\u001b[0;32m   2246\u001b[0m     new_values \u001b[38;5;241m=\u001b[39m bvals2[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39m_concat_same_type(bvals2, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m   2248\u001b[0m argsort \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margsort(new_mgr_locs)\n\u001b[1;32m-> 2249\u001b[0m new_values \u001b[38;5;241m=\u001b[39m \u001b[43mnew_values\u001b[49m\u001b[43m[\u001b[49m\u001b[43margsort\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m   2250\u001b[0m new_mgr_locs \u001b[38;5;241m=\u001b[39m new_mgr_locs[argsort]\n\u001b[0;32m   2252\u001b[0m bp \u001b[38;5;241m=\u001b[39m BlockPlacement(new_mgr_locs)\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 419. MiB for an array with shape (13, 4223651) and data type float64"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x1000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# correlation matrix and statistics of preprocessed data\n",
    "# for coorelation matrix we will use pearson correlation coefficient \n",
    "# for statistics we will use describe function,\n",
    "\n",
    "# correlation matrix without columns mapped_veh_id and timestamps_UTC\n",
    "corr_matrix = merged_df.drop(columns=['mapped_veh_id', 'timestamps_UTC']).corr(method='pearson')\n",
    "print(corr_matrix)\n",
    "\n",
    "# statistics of the data\n",
    "print(merged_df.describe())\n",
    "\n",
    "# plot the correlation matrix\n",
    "plt.figure(figsize=(10, 10))\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm')\n",
    "plt.title('Correlation Matrix')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
