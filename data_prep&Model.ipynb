{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask as da\n",
    "import dask.dataframe as daskdf\n",
    "import dask.array as daa\n",
    "import dask.distributed as dd\n",
    "import dask.datasets as ds\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "import seaborn as sns\n",
    "import sklearn as sk\n",
    "import numpy as np\n",
    "import dask_ml.preprocessing as dm_pre\n",
    "import dask_ml.cluster as dm_cluster\n",
    "import dask_geopandas as dg\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import geopandas as gpd\n",
    "from scipy.stats import pearsonr\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Data loaded in 0.07163572311401367 seconds\n"
     ]
    }
   ],
   "source": [
    "\"\"\"data look like this: ;mapped_veh_id;timestamps_UTC;lat;lon;RS_E_InAirTemp_PC1;RS_E_InAirTemp_PC2;RS_E_OilPress_PC1;RS_E_OilPress_PC2;RS_E_RPM_PC1;RS_E_RPM_PC2;RS_E_WatTemp_PC1;RS_E_WatTemp_PC2;RS_T_OilTemp_PC1;RS_T_OilTemp_PC2\n",
    "0;181;2023-08-01 03:44:12;50.7698183;3.8721144;27.0;23.0;255.0;238.0;794.0;801.0;83.0;81.0;76.0;77.0\n",
    "1;143;2023-08-01 06:36:29;51.0399934;3.6934285;33.0;32.0;272.0;324.0;802.0;804.0;78.0;78.0;73.0;74.0\n",
    "2;183;2023-08-24 06:53:54;50.7422026;3.6020347;31.0;33.0;234.0;182.0;799.0;802.0;82.0;82.0;85.0;87.0\"\"\"\n",
    "# Load data\n",
    "def load_data():\n",
    "    print(\"Loading data...\")\n",
    "    start = time.time()\n",
    "    df = daskdf.read_csv(\"ar41_for_ulb.csv\", sep=';')\n",
    "    end = time.time()\n",
    "    print(\"Data loaded in {} seconds\".format(end-start))\n",
    "    return df\n",
    "\n",
    "ddf = load_data()\n",
    "ddfprepprep = ddf.drop(columns=['Unnamed: 0'])\n",
    "# convert all temperatures from celcius to kelvin\n",
    "ddf['RS_E_InAirTemp_PC1'] = ddf['RS_E_InAirTemp_PC1'] + 273.15\n",
    "ddf['RS_E_InAirTemp_PC2'] = ddf['RS_E_InAirTemp_PC2'] + 273.15\n",
    "ddf['RS_E_WatTemp_PC1'] = ddf['RS_E_WatTemp_PC1'] + 273.15\n",
    "ddf['RS_E_WatTemp_PC2'] = ddf['RS_E_WatTemp_PC2'] + 273.15\n",
    "ddf['RS_T_OilTemp_PC1'] = ddf['RS_T_OilTemp_PC1'] + 273.15\n",
    "ddf['RS_T_OilTemp_PC2'] = ddf['RS_T_OilTemp_PC2'] + 273.15\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 3: Data Preparation\n",
    "This phase covers constructing the final data set for modeling tools. Steps include selecting data, cleansing data, constructing data, integrating data, and formatting data.\n",
    "\n",
    "Select Data:\n",
    "\n",
    "Decide on data based on relevance, quality, and technical constraints.\n",
    "Explain data inclusions/exclusions and prioritize attributes.\n",
    "Clean Data:\n",
    "\n",
    "Ensure data cleanliness, address missing values using appropriate techniques.\n",
    "Document how quality problems were addressed.\n",
    "Construct Data:\n",
    "\n",
    "Develop new records or derived attributes, considering modeling needs.\n",
    "Example: Create \"income per head\" as a derived attribute.\n",
    "Integrate Data:\n",
    "\n",
    "Combine information from multiple tables or records.\n",
    "Perform aggregations to summarize information.\n",
    "Format Data:\n",
    "\n",
    "Change data format or design to suit modeling tools.\n",
    "Example: Trim strings, reorganize information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select Data: Filtering out all data before january 2023\n",
    "def select_data(df):\n",
    "    ddf = df[df['timestamps_UTC'] >= '2023-01-01']\n",
    "    # to drop the erratic data and keep only those in 10-90 percentile for every feature\n",
    "    for col in ddf.columns:\n",
    "        if col not in ['timestamps_UTC', 'lat', 'lon', 'mapped_veh_id']:\n",
    "            ddf = ddf[(ddf[col] > ddf[col].quantile(0.0001)) & (ddf[col] < ddf[col].quantile(0.9999))]\n",
    "\n",
    "    return ddf\n",
    "\n",
    "ddfprep = select_data(ddf)\n",
    "\n",
    "# filter out all data out of belgium\n",
    "def filter_belgium(df):\n",
    "    ddf = df[(df['lat'] > 50.0) & (df['lat'] < 51.59) & (df['lon'] > 3.2) & (df['lon'] < 5.89)]\n",
    "    return ddf\n",
    "\n",
    "ddfprep = filter_belgium(ddfprep)\n",
    "#index fix \n",
    "ddfprep = ddfprep.reset_index(drop=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "## AS seen in exploratory analysis, we have nan values only for PC2 on 12726 rows over 17M rows so we can drop them\n",
    "ddfprep = ddfprep.drop_duplicates()\n",
    "ddfprep = ddfprep.dropna()\n",
    "\n",
    "#print (len (ddfprep)) \n",
    "#print(ddf.head())\n",
    "\n",
    "## TODO: Construct Data if needed here "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will cluster the lan and lon and associate to each row the name of a city centroid of the cluster \n",
    "def categorize_city(df):\n",
    "    conditions = [\n",
    "        (df['lat'] < 50.522345) & (df['lon'] < 4.586875),  # Charleroi\n",
    "        (df['lat'] > 50.522345) & (df['lon'] < 4.073960),  # Gent\n",
    "        ((df['lat'] >= 50.522345) & (df['lat'] <= 51.042345) & (4.073960 <= df['lon']) & (df['lon'] < 4.786476) ),  # Brussels\n",
    "        ((df['lat'] > 51.042345) & (4.073960 < df['lon']) & (df['lon'] < 4.786476) ), # Antwerp\n",
    "        ((df['lat'] >= 50.522345) & (4.786476 <= df['lon'])), # Hasselt\n",
    "    ]\n",
    "    choices = ['Charleroi', 'Gent', 'Brussels', 'Antwerp', 'Hasselt']\n",
    "    df['city'] = np.select(conditions, choices, default='Namur')  # Namur as default\n",
    "    return df\n",
    "\n",
    "# Apply the function to each partition of the Dask DataFrame\n",
    "ddfprep = ddfprep.map_partitions(categorize_city)\n",
    "\n",
    "ddfprep = ddfprep.drop_duplicates()\n",
    "ddfprep = ddfprep.dropna()\n",
    "# if column unnamed: 0 is present, drop it\n",
    "if 'Unnamed: 0' in ddfprep.columns:\n",
    "    ddfprep = ddfprep.drop(columns=['Unnamed: 0'])\n",
    "    \n",
    "print(ddfprep.head())\n",
    "print(len(ddfprep))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading external data...\n",
      "External data loaded in 0.0612027645111084 seconds\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "Dask dataframe does not yet support multi-indexes.\nYou tried to index with this index: ['city', 'date']\nIndexes must be single columns only.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[47], line 20\u001b[0m\n\u001b[0;32m     18\u001b[0m ddfprep[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdate\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m ddfprep[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtimestamps_UTC\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mdt\u001b[38;5;241m.\u001b[39mdate\n\u001b[0;32m     19\u001b[0m ddfprep[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdate\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m daskdf\u001b[38;5;241m.\u001b[39mto_datetime(ddfprep[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdate\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m---> 20\u001b[0m ddfprep \u001b[38;5;241m=\u001b[39m \u001b[43mddfprep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcity\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdate\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     22\u001b[0m external_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdate\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m daskdf\u001b[38;5;241m.\u001b[39mto_datetime(external_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdate\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     23\u001b[0m external_df \u001b[38;5;241m=\u001b[39m external_df\u001b[38;5;241m.\u001b[39mset_index([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcity\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdate\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\dask\\dataframe\\core.py:5380\u001b[0m, in \u001b[0;36mDataFrame.set_index\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m   5378\u001b[0m         other \u001b[38;5;241m=\u001b[39m other[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   5379\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 5380\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[0;32m   5381\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDask dataframe does not yet support multi-indexes.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   5382\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou tried to index with this index: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mother\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   5383\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIndexes must be single columns only.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   5384\u001b[0m         )\n\u001b[0;32m   5386\u001b[0m \u001b[38;5;66;03m# Or be a frame directly\u001b[39;00m\n\u001b[0;32m   5387\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(other, DataFrame):  \u001b[38;5;66;03m# type: ignore[unreachable]\u001b[39;00m\n",
      "\u001b[1;31mNotImplementedError\u001b[0m: Dask dataframe does not yet support multi-indexes.\nYou tried to index with this index: ['city', 'date']\nIndexes must be single columns only."
     ]
    }
   ],
   "source": [
    "# Integrate external data \n",
    "# here we will join external weather data with our data \n",
    "# for that we will perform join over the city and timestamp.\n",
    "\n",
    "# Load external data\n",
    "def load_external_data():\n",
    "    print(\"Loading external data...\")\n",
    "    start = time.time()\n",
    "    df = daskdf.read_csv(\"weather.csv\")\n",
    "    end = time.time()\n",
    "    print(\"External data loaded in {} seconds\".format(end-start))\n",
    "    return df\n",
    "\n",
    "external_df = load_external_data()\n",
    "\n",
    "\n",
    "# Prepare for merge by creating a 'date' column\n",
    "ddfprep['date'] = dd.to_datetime(ddfprep['timestamps_UTC']).dt.date\n",
    "external_df['date'] = dd.to_datetime(external_df['date'])\n",
    "\n",
    "external_df['date'] = daskdf.to_datetime(external_df['date'])\n",
    "external_df = external_df.set_index(['city', 'date'])\n",
    "\n",
    "# Merge the DataFrames on 'city' and 'date'\n",
    "merged_df = ddfprep.merge(external_df, on=['city', 'date'], how='left')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look for nan values\n",
    "if ddfprep.isnull().values.any():\n",
    "    print(\" nan finded  will be filled with the mean of the column\")\n",
    "    ddfprep = ddfprep.fillna(ddfprep.mean())\n",
    "\n",
    "# Drop columns that are not needed anymore\n",
    "columns_to_drop = ['lat', 'lon', 'date', 'hour', 'city', 'longitude', 'latitude']\n",
    "merged_df = merged_df.drop(columns=columns_to_drop)\n",
    "\n",
    "if ddfprep.duplicated().values.any():\n",
    "    print(\"duplicates finded  will be dropped\")\n",
    "    ddfprep = ddfprep.drop_duplicates()\n",
    "\n",
    "# index fix\n",
    "ddfprep = ddfprep.reset_index(drop=True)\n",
    "\n",
    "# show results\n",
    "print(ddfprep.head())\n",
    "\n",
    "# save the preprocessed data\n",
    "ddfprep.to_csv('preprocessed_data.csv')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correlation matrix and statistics of preprocessed data\n",
    "# for coorelation matrix we will use pearson correlation coefficient and avoid column : mapped_veh_id, timestamps_UTC,\n",
    "# for statistics we will use describe function,\n",
    "corr = ddfprep.corr()\n",
    "print(corr)\n",
    "print(ddfprep.describe())\n",
    "\n",
    "# plot correlation matrix\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(corr, annot=True, cmap=plt.cm.Reds)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase Four: Modeling. \n",
    "This phase involves selecting, applying, and tuning various statistical or machine learning models to your prepared data. Here's what typically happens in this phase:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modeling techniques selection\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
